# 实验一
## 题目一
1. **进退法原理回顾**
    - 进退法用于确定单变量函数的一个包含极小值点的区间。它的基本思想是从一个初始点出发，按照一定的步长前进或后退，直到找到函数值开始上升的点，这样就确定了一个包含极小值点的区间。
    - 算法步骤如下：
        - 从初始点\(x_0\)出发，按照步长\(h\)前进得到\(x_1 = x_0+h\)。
        - 比较\(f(x_1)\)和\(f(x_0)\)：
            - 如果\(f(x_1) \geq f(x_0)\)，则可能需要改变步长方向。如果这是第一次迭代，将步长变为\(-h\)并重新计算\(x_1\)；否则，停止迭代。
            - 如果\(f(x_1) \lt f(x_0)\)，则增加步长（通常是加倍），更新\(x_0\)和\(f(x_0)\)，继续迭代。
        - 最后，找到的两个点就是包含极小值点的区间的左右端点。
2. **代码实现分析**
    - **函数定义**
        - `double f(double x)`：定义了要优化的目标函数。
    - **进退法函数 `Brackeing`**
        - **变量声明**
            - `const double a = 2.0;`：定义了步长加倍系数。
            - `int k = 1;`：迭代计数器。
            - `double lambda = 0.0;`：用于记录临时值。
            - `int flag = 0;`：用于标记是否找到区间。
            - `double x[11];`和`double f_val[11];`：分别用于存储自变量和函数值。
        - **初始化**
            - `x[1] = x0;`和`f_val[1] = func(x[1]);`：初始化自变量和函数值。
        - **迭代过程**
            - 在`while (!flag)`循环中：
                - **试探和计算**
                    - `x[k + 1] = x[k] + h;`和`f_val[k + 1] = func(x[k + 1]);`：计算下一个点及其函数值。
                - **比较**
                    - 如果`f_val[k + 1] >= f_val[k]`：
                        - 如果`k == 1`，则改变步长方向：`h = -h; lambda = x[k + 1]; x[k + 1] = x[k] + h; f_val[k + 1] = func(x[k + 1]);`
                        - 否则，设置`flag = 1;`以结束迭代。
                - **加步**
                    - 如果`flag`为`false`，则加倍步长：`h = a * h; lambda = x[k]; x[k] = x[k + 1]; f_val[k] = f_val[k + 1]; k++;`
        - **确定区间**
            - 最后，`*xmin = fmin(lambda, x[k + 1]);`和`*xmax = fmax(lambda, x[k + 1]);`确定包含极小值点的区间。
    - **主函数 `main`**
        - 初始化`xmin`、`xmax`、`x0`和`h`。
        - 调用`Brackeing`函数找到区间。
        - 输出区间的左右端点。

这段代码通过`Brackeing`函数实现了进退法的步骤，用于找到单变量函数的一个包含极小值点的区间。

## 题目二、题目三
1. **黄金分割法（0.618法）原理回顾**
    - 黄金分割法是一种用于单变量函数优化的方法，它通过迭代地缩小包含最小值的区间来找到函数的最小值。
    - 基本思想是在每次迭代中，通过比较两个内部点（\(\lambda\)和\(\mu\)）的函数值，决定丢弃区间的哪一部分，从而不断缩小搜索区间。
    - 算法步骤如下：
        - 确定初始搜索区间\([a_0, b_0]\)和容差\(\epsilon\)，计算\(\tau = (\sqrt{5}-1)/2\)，并计算初始试探点\(\lambda_0 = a_0+(1-\tau)(b_0 - a_0)\)和\(\mu_0 = a_0+\tau(b_0 - a_0)\)。
        - 比较\(\phi(\lambda_i)\)和\(\phi(\mu_i)\)：
            - 如果\(\phi(\lambda_i) \leq \phi(\mu_i)\)，则令\(b_{i+1} = \mu_i\)，\(\mu_{i+1} = \lambda_i\)，\(\lambda_{i+1} = a_{i+1}+(1-\tau)(b_{i+1}-a_{i+1})\)。
            - 如果\(\phi(\lambda_i) > \phi(\mu_i)\)，则令\(a_{i+1} = \lambda_i\)，\(\lambda_{i+1} = \mu_i\)，\(\mu_{i+1} = a_{i+1}+\tau(b_{i+1}-a_{i+1})\)。
        - 重复上述步骤，直到\(b_i - a_i \leq \epsilon\)。
2. **代码实现分析**
    - **函数定义**
        - `double f(double x)`：定义了要优化的目标函数。
    - **黄金分割法函数 `MinHJ`**
        - **变量声明**
            - `double tau = 0.618;`：定义了黄金分割比例。
            - `double lambda, mu, lambda_val, mu_val;`：用于存储试探点及其函数值。
        - **初始化**
            - `lambda = a + (1 - tau) * (b - a);`和`mu = a + tau * (b - a);`：计算初始试探点。
            - `lambda_val = func(lambda);`和`mu_val = func(mu);`：计算初始试探点的函数值。
        - **迭代过程**
            - 在`while (b - a >= eps)`循环中：
                - 如果`lambda_val < mu_val`：
                    - 更新区间和试探点：`b = mu; mu = lambda; mu_val = lambda_val; lambda = a + (1 - tau) * (b - a); lambda_val = func(lambda);`
                - 如果`lambda_val >= mu_val`：
                    - 更新区间和试探点：`a = lambda; lambda = mu; lambda_val = mu_val; mu = a + tau * (b - a); mu_val = func(mu);`
        - **计算结果**
            - `*x = (lambda + mu) / 2;`和`*miny = func(*x);`：计算最终的最小值点和最小值。
    - **主函数 `main`**
        - 初始化`a`、`b`、`eps`、`x`和`miny`。
        - 调用`MinHJ`函数找到最小值点和最小值。
        - 输出最小值点和最小值。

这段代码通过`MinHJ`函数实现了黄金分割法（0.618法）的步骤，用于找到单变量函数的最小值。

# 0.618法（黄金分割搜索法）

## 算法思想:

0.618法是一种用于在一维空间内寻找函数最小值的优化方法。它通过不断缩小区间的方式逼近最优解。该方法利用了黄金比例的性质来选择区间的两个测试点，从而保证每次迭代都能有效地缩小搜索范围。

## 步骤:

1. 初始化区间[a, b]。
2. 计算初始的两个内部点$x_1 = a + \frac{0.618}{b-a}(b - a)$ 和 $x_2 = b - \frac{0.618}{b-a}(b - a)$。
3. 比较这两个点的函数值$f(x_1)$和$f(x_2)$。
4. 根据比较结果，去掉包含较大函数值的那一个点所在的区间的一半。
5. 在新的区间上重复上述过程，直到满足停止条件（如达到指定的精度）。

## 抛物线法

## 算法思想:

抛物线法是通过构造一个二次插值多项式来近似目标函数，然后在该多项式的顶点处找到可能的极小点。这种方法假设函数在某三个连续点附近的行为可以用一个抛物线来近似。

## 步骤:

1. 选择三个初始点$x_0$, $x_1$, $x_2$，其中$x_0 < x_1 < x_2$。
2. 构造经过这三个点的二次插值多项式$p(x) = ax^2 + bx + c$。
3. 找到该抛物线的顶点$x_{min} = -\frac{b}{2a}$作为当前的最低点估计。
4. 如果$x_{min}$不在当前的三点之间，则根据其与边界的关系更新最低点估计。
5. 更新三个点中的某一个以进一步细化搜索区域。
6. 重复上述过程，直到满足停止条件。
